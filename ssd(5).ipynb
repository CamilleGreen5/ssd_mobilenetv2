{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "ssd.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YQRdG6ZiP0uB",
        "oa_IJurN31td"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP3yCUbowxPG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow as imshow"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQRdG6ZiP0uB"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g0ArMdGP3cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f35468-4509-4e07-d9f3-46f2a6f5a352"
      },
      "source": [
        "!rm -r sample_data"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql4gda8n3ejH"
      },
      "source": [
        "# !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "# !wget http://images.cocodataset.org/zips/val2017.zip\n",
        "# !unzip annotations_trainval2017.zip\n",
        "# !unzip val2017.zip\n",
        "# !rm annotations_trainval2017.zip\n",
        "# !rm val2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa_IJurN31td"
      },
      "source": [
        "## Coco Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6VUQ4E935C5"
      },
      "source": [
        "class COCODataset():\n",
        "    \"\"\" Class for COCO Dataset\n",
        "\n",
        "    Attributes:\n",
        "        root_dir: dataset root dir (ex: ./data/VOCdevkit)\n",
        "        num_examples: number of examples to be used\n",
        "                      (in case one wants to overfit small data)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, year, default_boxes,\n",
        "                 new_size, liste_obj, num_examples=-1, clear_data=False, augmentation=None):\n",
        "        super(COCODataset, self).__init__()\n",
        "        self.idx_to_name = liste_obj\n",
        "        self.name_to_idx = dict([(v, k)\n",
        "                                 for k, v in enumerate(self.idx_to_name)])\n",
        "        self.root_dir = root_dir\n",
        "        self.image_dir = os.path.join(self.root_dir, 'Images')\n",
        "        self.anno_dir = os.path.join(self.root_dir, 'Annotations')\n",
        "        self.annotation_filename = os.path.join(self.anno_dir, 'instances_val2017.json')\n",
        "        self.ids = list(map(lambda x: x[:-4], os.listdir(self.image_dir)))  # name de toutes les images\n",
        "        self.default_boxes = default_boxes\n",
        "        self.new_size = new_size\n",
        "\n",
        "        list_pop = []\n",
        "        for index in range(len(self.ids)):\n",
        "            img = self._get_image(index)\n",
        "            np_img = np.array(img)\n",
        "            s = np_img.shape\n",
        "            if len(s) != 3:\n",
        "                list_pop.append(index)\n",
        "\n",
        "        if num_examples != -1:\n",
        "            self.ids = self.ids[:num_examples]\n",
        "\n",
        "        if clear_data:\n",
        "            for index in range(len(self.ids)):\n",
        "                img = self._get_image(index)\n",
        "                w, h = img.size\n",
        "                _, labels = self._get_annotation(index, (h, w))\n",
        "                nb = [0 for i in range(len(self.idx_to_name)+1)]\n",
        "                for label in labels:\n",
        "                    if 0 < label < len(self.idx_to_name) + 1:\n",
        "                        nb[label] += 1\n",
        "                if not (np.array(nb).any() != 0):\n",
        "                    list_pop.append(index)\n",
        "        for i in range(len(list_pop)-1, 0, -1):\n",
        "            a = (self.ids).pop(list_pop[i])\n",
        "            # print(a)\n",
        "\n",
        "        if num_examples != -1:\n",
        "            self.ids = self.ids[:num_examples]\n",
        "\n",
        "        self.train_ids = self.ids[:int(len(self.ids) * 0.75)]\n",
        "        self.val_ids = self.ids[int(len(self.ids) * 0.75):]\n",
        "\n",
        "        if augmentation is None:\n",
        "            self.augmentation = ['original']\n",
        "        else:\n",
        "            self.augmentation = augmentation + ['original']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def _get_image(self, index):\n",
        "        \"\"\" Method to read image from file\n",
        "            then resize to (300, 300)\n",
        "            then subtract by ImageNet's mean\n",
        "            then convert to Tensor\n",
        "\n",
        "        Args:\n",
        "            index: the index to get filename from self.ids\n",
        "\n",
        "        Returns:\n",
        "            img: tensor of shape (3, 300, 300)\n",
        "        \"\"\"\n",
        "        filename = self.ids[index]\n",
        "        img_path = os.path.join(self.image_dir, filename + '.jpg')\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _get_annotation(self, index, orig_shape):\n",
        "        \"\"\" Method to read annotation from file\n",
        "            Boxes are normalized to image size\n",
        "            Integer labels are increased by 1\n",
        "\n",
        "        Args:\n",
        "            index: the index to get filename from self.ids\n",
        "            orig_shape: image's original shape\n",
        "\n",
        "        Returns:\n",
        "            boxes: numpy array of shape (num_gt, 4)\n",
        "            labels: numpy array of shape (num_gt,)\n",
        "        \"\"\"\n",
        "        h, w = orig_shape\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        image_id = 0\n",
        "\n",
        "        with open(self.annotation_filename, 'r') as COCO:\n",
        "            coco = json.loads(COCO.read())\n",
        "\n",
        "        filename = self.ids[index]\n",
        "        img_path = str(filename) + '.jpg'\n",
        "\n",
        "        for img in coco['images']:\n",
        "            if img['file_name'] == img_path:\n",
        "                image_id = img['id']\n",
        "\n",
        "        for obj in coco['annotations']:\n",
        "            if obj['image_id'] == image_id:\n",
        "                idx = obj['category_id']\n",
        "                bbox = obj['bbox']\n",
        "                xmin = (float(obj['bbox'][0]) - 1) / w\n",
        "                ymin = (float(obj['bbox'][1]) - 1) / h\n",
        "                xmax = (float(obj['bbox'][2]) - 1) / w\n",
        "                ymax = (float(obj['bbox'][3]) - 1) / h\n",
        "                boxes.append([xmin, ymin, xmin+xmax, ymin+ymax])\n",
        "                nb = 0\n",
        "                for categorie in self.idx_to_name:\n",
        "                    id_c = self.name_to_idx[categorie] + 1\n",
        "                    if id_c == idx:\n",
        "                        nb += 1\n",
        "                        labels.append(self.name_to_idx[categorie]+1)\n",
        "                if nb == 0:\n",
        "                    labels.append(0)\n",
        "        return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
        "\n",
        "    def generate(self, subset=None):\n",
        "        \"\"\" The __getitem__ method\n",
        "            so that the object can be iterable\n",
        "\n",
        "        Args:\n",
        "            index: the index to get filename from self.ids\n",
        "\n",
        "        Returns:\n",
        "            img: tensor of shape (300, 300, 3)\n",
        "            boxes: tensor of shape (num_gt, 4)\n",
        "            labels: tensor of shape (num_gt,)\n",
        "        \"\"\"\n",
        "        if subset == 'train':\n",
        "            indices = self.train_ids\n",
        "        elif subset == 'val':\n",
        "            indices = self.val_ids\n",
        "        else:\n",
        "            indices = self.ids\n",
        "        for index in range(len(indices)):\n",
        "            filename = indices[index]\n",
        "            # img, orig_shape = self._get_image(index)\n",
        "            img = self._get_image(index)\n",
        "            w, h = img.size\n",
        "            boxes, labels = self._get_annotation(index, (h, w))\n",
        "            boxes = tf.constant(boxes, dtype=tf.float32)\n",
        "            labels = tf.constant(labels, dtype=tf.int64)\n",
        "\n",
        "            augmentation_method = np.random.choice(self.augmentation)\n",
        "            if augmentation_method == 'patch':\n",
        "                img, boxes, labels = random_patching(img, boxes, labels)\n",
        "            elif augmentation_method == 'flip':\n",
        "                img, boxes, labels = horizontal_flip(img, boxes, labels)\n",
        "\n",
        "            img = np.array(img.resize(\n",
        "                (self.new_size, self.new_size)), dtype=np.float32)\n",
        "            img = (img / 127.0) - 1.0\n",
        "            img = tf.constant(img, dtype=tf.float32)\n",
        "\n",
        "            gt_confs, gt_locs = compute_target(\n",
        "                self.default_boxes, boxes, labels)\n",
        "\n",
        "            labels = labels.numpy()\n",
        "            labels = labels.squeeze()\n",
        "            sum_lab = 0\n",
        "            for j in range(1, len(self.idx_to_name)+1):\n",
        "                lab_obj = labels[labels == j]\n",
        "                sum_lab_obj = sum(lab_obj)\n",
        "                sum_lab = sum_lab + sum_lab_obj\n",
        "\n",
        "            yield filename, img, gt_confs, gt_locs, sum_lab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgt074Rl3_P-"
      },
      "source": [
        "def create_batch_generator_coco(root_dir, year, default_boxes,\n",
        "                           new_size, batch_size, num_batches,\n",
        "                           mode, liste_obj, clear_data,\n",
        "                           augmentation=None):\n",
        "    num_examples = batch_size * num_batches if num_batches > 0 else -1\n",
        "    voc = COCODataset(root_dir, year, default_boxes,\n",
        "                     new_size, liste_obj, num_examples, clear_data, augmentation)\n",
        "\n",
        "    info = {\n",
        "        'idx_to_name': voc.idx_to_name,\n",
        "        'name_to_idx': voc.name_to_idx,\n",
        "        'length': len(voc),\n",
        "        'image_dir': voc.image_dir,\n",
        "        'anno_dir': voc.anno_dir\n",
        "    }\n",
        "\n",
        "    if mode == 'train':     # separe en Train et Val dataset + shuffle et batch, renvoit tf dataset\n",
        "        train_gen = partial(voc.generate, subset='train')\n",
        "        train_dataset = tf.data.Dataset.from_generator(\n",
        "            train_gen, (tf.string, tf.float32, tf.int64, tf.float32, tf.int64))\n",
        "        val_gen = partial(voc.generate, subset='val')\n",
        "        val_dataset = tf.data.Dataset.from_generator(\n",
        "            val_gen, (tf.string, tf.float32, tf.int64, tf.float32, tf.int64))\n",
        "\n",
        "        train_dataset = train_dataset.shuffle(40).batch(batch_size)\n",
        "        val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "        return train_dataset.take(num_batches), val_dataset.take(-1), info\n",
        "    else:\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            voc.generate, (tf.string, tf.float32, tf.int64, tf.float32, tf.int64))\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        return dataset.take(num_batches), info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY80OFr3Piwz"
      },
      "source": [
        "## SSD MOBILENET MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzXxkPItwxPW"
      },
      "source": [
        "official_mobilenet = tf.keras.applications.MobileNetV2((224, 224, 3))\n",
        "\n",
        "official_mobilenet.build(((224, 224, 3)))\n",
        "# official_mobilenet.summary()\n",
        "# ssd = tf.keras.models.clone_model(\n",
        "#     official_mobilenet, input_tensors=None, clone_function=None)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqzgwdOkeWDg"
      },
      "source": [
        "class SSD(tf.keras.Model):\n",
        "\n",
        "\n",
        "  def __init__(self, source_model, numb_classes, num_of_layers_to_clone=None):\n",
        "\n",
        "    super(SSD, self).__init__()\n",
        "    # CLONING LAYERS OF SOURCE MODEL\n",
        "    for i, layer in enumerate(source_model.layers):\n",
        "      if num_of_layers_to_clone is not None and i <= num_of_layers_to_clone:\n",
        "        setattr(self, layer.name, layer)\n",
        "      elif num_of_layers_to_clone is None :\n",
        "        setattr(self, layer.name, layer)\n",
        "    self.copy_weights(source_model)\n",
        "\n",
        "    # SETTING ATTRIBUTES\n",
        "    self.num_classes = numb_classes\n",
        "    self.input_size = [224, 224, 3]\n",
        "    self.indice_ADD = [27, 45, 54, 72, 81, 90, 107, 116, 134, 143]\n",
        "    self.indice_copy_for_ADD = [18, 36, 63, 72, 81, 98, 107, 116, 125, 134]\n",
        "    self.indice_OUTPUT = [54, 90, 151, 159, 167]\n",
        "    self.mobilenet_layers = np.arange(num_of_layers_to_clone+1)\n",
        "    self.ssd_layers = np.arange(num_of_layers_to_clone, 168)\n",
        "    self.head_layers = np.arange(169, len(self.layers))\n",
        "    self.default_boxes_sizes = [28, 14, 7, 4, 1]\n",
        "    self.add_ssd_layer()\n",
        "\n",
        "\n",
        "  # TO COPY WEIGHTS OF SOURCE MODEL\n",
        "  def copy_weights(self, source_model):\n",
        "    assert len(source_model.layers) >= len(self.layers)\n",
        "    for i in range(len(self.layers)):\n",
        "      self.layers[i].set_weights(source_model.layers[i].get_weights())\n",
        "\n",
        "\n",
        "  # ADD SSD EXTRA LAYERS\n",
        "  def add_ssd_layer(self):\n",
        "    \n",
        "    # block 17\n",
        "    self.block_17_expand = tf.keras.layers.Conv2D(320*3, (1, 1), padding='same', name='block_17_expand')\n",
        "    self.block_17_expand_BN = tf.keras.layers.BatchNormalization(name='block_17_expand_BN')\n",
        "    self.block_17_expand_relu = tf.keras.layers.ReLU(6, name='block_17_expand_relu')\n",
        "    self.block_17_depthwise = tf.keras.layers.DepthwiseConv2D((3, 3), strides=(2,2), padding='same', name='block_17_depthwise')\n",
        "    self.block_17_depthwise_BN = tf.keras.layers.BatchNormalization(name='block_17_depthwise_BN')\n",
        "    self.block_17_depthwise_relu = tf.keras.layers.ReLU(6, name='block_17_depthwise_relu')\n",
        "    self.block_17_project = tf.keras.layers.Conv2D(320, (1, 1), padding='same', name='block_17_project')\n",
        "    self.block_17_project_BN = tf.keras.layers.BatchNormalization(name='block_17_project_BN')\n",
        "\n",
        "    # block 18\n",
        "    self.block_18_expand = tf.keras.layers.Conv2D(320*3, (1, 1), padding='same', name='block_18_expand')\n",
        "    self.block_18_expand_BN = tf.keras.layers.BatchNormalization(name='block_18_expand_BN')\n",
        "    self.block_18_expand_relu = tf.keras.layers.ReLU(6, name='block_18_expand_relu')\n",
        "    self.block_18_depthwise = tf.keras.layers.DepthwiseConv2D((3, 3), strides=(2,2), padding='valid', name='block_18_depthwise')\n",
        "    self.block_18_depthwise_BN = tf.keras.layers.BatchNormalization(name='block_18_depthwise_BN')\n",
        "    self.block_18_depthwise_relu = tf.keras.layers.ReLU(6, name='block_18_depthwise_relu')\n",
        "    self.block_18_project = tf.keras.layers.Conv2D(320, (1, 1), padding='same', name='block_18_project')\n",
        "    self.block_18_project_BN = tf.keras.layers.BatchNormalization(name='block_18_project_BN')\n",
        "\n",
        "    # conf head layers\n",
        "    self.conf_head_layers_0 = tf.keras.layers.Conv2D(self.num_classes, kernel_size=3, padding='same', name='conf_head_layers_0')  # for output_0\n",
        "    self.conf_head_layers_1 = tf.keras.layers.Conv2D(self.num_classes, kernel_size=3, padding='same', name='conf_head_layers_1')  # for output_1\n",
        "    self.conf_head_layers_2 = tf.keras.layers.Conv2D(self.num_classes, kernel_size=3, padding='same', name='conf_head_layers_2')  # for output_2\n",
        "    self.conf_head_layers_3 = tf.keras.layers.Conv2D(self.num_classes, kernel_size=3, padding='same', name='conf_head_layers_3')  # for output_3\n",
        "    self.conf_head_layers_4 = tf.keras.layers.Conv2D(self.num_classes, kernel_size=1, padding='same', name='conf_head_layers_4')  # for output_4\n",
        "\n",
        "    # loc_head_layers\n",
        "    self.loc_head_layers_0 = tf.keras.layers.Conv2D(4, kernel_size=3, padding='same', name='loc_head_layers_0')\n",
        "    self.loc_head_layers_1 = tf.keras.layers.Conv2D(4, kernel_size=3, padding='same', name='loc_head_layers_1')\n",
        "    self.loc_head_layers_2 = tf.keras.layers.Conv2D(4, kernel_size=3, padding='same', name='loc_head_layers_2')\n",
        "    self.loc_head_layers_3 = tf.keras.layers.Conv2D(4, kernel_size=3, padding='same', name='loc_head_layers_3')\n",
        "    self.loc_head_layers_4 = tf.keras.layers.Conv2D(4, kernel_size=1, name='loc_head_layers_4')\n",
        "\n",
        "\n",
        "  def compute_output_shape(self):\n",
        "    input_shape = [224, 224, 3]\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      if np.isin(i, self.mobilenet_layers):\n",
        "        self.layers[i]._output_shape_ = layer.output_shape[1:]\n",
        "        input_shape = layer._output_shape_\n",
        "      elif np.isin(i, self.ssd_layers) and layer.name[-3] != 'add':\n",
        "        x = np.zeros(input_shape, dtype=np.float32)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = layer(x)\n",
        "        self.layers[i]._output_shape_ = x.shape[1:]\n",
        "        input_shape = layer._output_shape_\n",
        "\n",
        "\n",
        "  def info_model(self):\n",
        "    for i, layer in enumerate(self.layers[:self.ssd_layers[-1]+1]):\n",
        "      a = str(i)\n",
        "      b = str(layer.name)\n",
        "      c = str(layer._output_shape_)\n",
        "      while(len(a) != 10):\n",
        "        a = a + ' '\n",
        "      while(len(b) <= 30):\n",
        "        b = b + ' '\n",
        "      info = a + b + c\n",
        "      print(info)\n",
        "\n",
        "\n",
        "  def freeze_layers(self, indices, unfreeze=False):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      if np.isin(i, indices) and not unfreeze:\n",
        "        self.layers[i].trainable = False\n",
        "      elif np.isin(i, indices) and unfreeze:\n",
        "        self.layers[i].trainable = True\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    for i, layer in enumerate(self.layers[:self.ssd_layers[-1]+1]):\n",
        "      \n",
        "      # Input\n",
        "      if i == 0:\n",
        "        x = layer(inputs)\n",
        "      # NOT ADD\n",
        "      if not np.isin(i, self.indice_ADD):\n",
        "        x = layer(x)\n",
        "      else:\n",
        "        x = layer([x, x1])\n",
        "      # COPY for ADD\n",
        "      if np.isin(i, self.indice_copy_for_ADD):\n",
        "        x1 = x\n",
        "      # OUTPUT\n",
        "      if np.isin(i, self.indice_OUTPUT):\n",
        "        output = x\n",
        "        if i==54:\n",
        "          conf_output = self.conf_head_layers_0(output)\n",
        "          loc_output = self.loc_head_layers_0(output)\n",
        "        elif i==90:\n",
        "          conf_output = self.conf_head_layers_1(output)\n",
        "          loc_output = self.loc_head_layers_1(output)\n",
        "        elif i==151:\n",
        "          conf_output = self.conf_head_layers_2(output)\n",
        "          loc_output = self.loc_head_layers_2(output)\n",
        "        elif i==159:\n",
        "          conf_output = self.conf_head_layers_3(output)\n",
        "          loc_output = self.loc_head_layers_3(output)\n",
        "        elif i==167:\n",
        "          conf_output = self.conf_head_layers_4(output)\n",
        "          loc_output = self.loc_head_layers_4(output)\n",
        "\n",
        "        conf_output = tf.reshape(conf_output, (conf_output.shape[0], -1, self.num_classes))\n",
        "        loc_output = tf.reshape(loc_output, (loc_output.shape[0], -1, 4))\n",
        "\n",
        "        # print(conf_output.shape)\n",
        "        # print(loc_output.shape, '\\n')\n",
        "        if i==54:\n",
        "          list_conf_output = conf_output\n",
        "          list_loc_output = loc_output\n",
        "        else:\n",
        "          list_conf_output = tf.concat([list_conf_output, conf_output], axis=1)\n",
        "          list_loc_output = tf.concat([list_loc_output, loc_output], axis=1)\n",
        "\n",
        "    list_conf_output = tf.squeeze(list_conf_output)\n",
        "    list_loc_output = tf.squeeze(list_loc_output)\n",
        "    return list_conf_output, list_loc_output\n",
        "\n",
        "\n",
        "  def compute_default_boxes(self):\n",
        "    \n",
        "    self.default_boxes_indices = [0]\n",
        "    for im_size in self.default_boxes_sizes:\n",
        "      self.default_boxes_indices.append(self.default_boxes_indices[-1] + im_size*im_size)\n",
        "\n",
        "    self.default_boxes_indices = np.array(self.default_boxes_indices)\n",
        "\n",
        "    original_im_size = self.input_size[0]\n",
        "    for i, im_size in enumerate(self.default_boxes_sizes):\n",
        "      default_boxes_part = np.zeros((im_size*im_size, 4), dtype=np.uint8)\n",
        "      cel_size = int(original_im_size / im_size)\n",
        "      for cel_x in range(im_size):\n",
        "        for cel_y in range(im_size):\n",
        "          default_boxes_part[cel_x*im_size+cel_y] = [cel_x*cel_size, (cel_x+1)*cel_size, cel_y*cel_size, (cel_y+1)*cel_size]\n",
        "      if i==0:\n",
        "        default_boxes = default_boxes_part\n",
        "      else:\n",
        "        default_boxes = np.concatenate([default_boxes, default_boxes_part], axis=0)\n",
        "    return default_boxes\n",
        "\n",
        "\n",
        "  def predict(self, input):\n",
        "    conf_pred, loc_pred = self(input)\n",
        "    default_boxes = self.pred_to_default_boxes(loc_pred)\n",
        "    classes = []\n",
        "    confs = []\n",
        "    index = []\n",
        "    for i, box in enumerate(default_boxes):\n",
        "      conf = tf.nn.softmax(conf_pred[i])\n",
        "      classe = np.argmax(conf)\n",
        "      conf = conf[classe]\n",
        "      classes.append(classe)\n",
        "      confs.append(conf)\n",
        "    for i, conf in enumerate(confs):\n",
        "      if conf>0.5 and classes[i] != 0:\n",
        "        index.append(i)\n",
        "    confs = np.array(confs)[index]\n",
        "    classes = np.array(classes)[index]\n",
        "    boxes = np.array(default_boxes)[index]\n",
        "\n",
        "    return boxes, classes, confs\n",
        "\n",
        "\n",
        "  def pred_to_default_boxes(self, loc_pred):\n",
        "    default_boxes = self.compute_default_boxes()\n",
        "    print(default_boxes.shape)\n",
        "    print(loc_pred.shape)\n",
        "    assert default_boxes.shape == loc_pred.shape\n",
        "    for i, box in enumerate(default_boxes):\n",
        "      default_boxes[i] = default_boxes[i] + loc_pred[i]\n",
        "      for pix_index in default_boxes[i]:\n",
        "        if pix_index > 224:\n",
        "          pix_index = 224\n",
        "        elif pix_index < 0:\n",
        "          pix_index = 0\n",
        "    return default_boxes\n",
        "\n",
        "  \n",
        "  def compute_nms(self, loc_pred, loc_label):\n",
        "    nms = 0\n",
        "    for box in loc_pred:\n",
        "    return nms\n",
        "\n",
        "\n",
        "  def compute_overlapp(box_a, box_b):\n",
        "  \n",
        "    if np.minimum(box_a[1], box_b[1]) - np.maximum(box_a[0], box_b[0]) > 0 and np.minimum(box_a[3], box_b[3]) - np.maximum(box_a[2], box_b[2]) > 0:\n",
        "      intersection = (np.minimum(box_a[1], box_b[1]) - np.maximum(box_a[0], box_b[0]))*(np.minimum(box_a[3], box_b[3]) - np.maximum(box_a[2], box_b[2]))\n",
        "      union = (box_a[1]-box_a[0])*(box_a[3]-box_a[2]) + (box_b[1]-box_b[0])*(box_b[3]-box_b[2])- intersection\n",
        "      overlapp = intersection / union\n",
        "    else:\n",
        "      overlapp=0\n",
        "\n",
        "    return overlapp\n",
        "\n",
        "\n",
        "  def see_default_box(self):\n",
        "    default_boxes = self.compute_default_boxes()\n",
        "    im = np.zeros((224, 224, 3))\n",
        "    for box in default_boxes[self.default_boxes_indices[3]:self.default_boxes_indices[4]]:\n",
        "    # print(box)\n",
        "      im = cv.rectangle(im, (box[0], box[2]), (box[1], box[3]), (255, 255, 255), 1)\n",
        "    imshow(im)"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N3r59dIfMNM"
      },
      "source": [
        "model = SSD(official_mobilenet, 2, 151)\n",
        "model.build((1, 224, 224, 3))\n",
        "model.compute_output_shape()"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5U3hLys3sMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4802ed68-8bef-4c3a-e635-0d8b3880ed2b"
      },
      "source": [
        "im = np.zeros((224, 224, 3), dtype= np.float32)\n",
        "im = np.expand_dims(im, axis=0)\n",
        "conf_pred, loc_pred = model(im)\n",
        "loc_pred = np.squeeze(loc_pred)\n",
        "print(loc_pred.shape)\n",
        "default_boxes = model.pred_to_default_boxes(loc_pred)\n",
        "print(default_boxes.shape)\n",
        "# print(default_boxes)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1046, 4)\n",
            "(1046, 4)\n",
            "(1046, 4)\n",
            "(1046, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELUJ0o7ImRXD"
      },
      "source": [
        "model.freeze_layers(model.ssd_layers)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4eCvF8IQV3s"
      },
      "source": [
        "model.info_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2m2YA7OPmBM"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP8wsLInwxPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13217133-48dd-4146-aa84-81fa2254f4c8"
      },
      "source": [
        "# im = cv.imread('cat.10.jpg')\n",
        "im = np.zeros((224, 224, 3), dtype= np.float32)\n",
        "im = np.expand_dims(im, axis=0)\n",
        "t0 = time.time()\n",
        "box_pred, classes_pred, confs_pred = model.predict(im)\n",
        "t1 = time.time()\n",
        "print('computation_time = ', t1-t0)\n",
        "# conf_pred = np.squeeze(conf_pred)\n",
        "# loc_pred = np.squeeze(loc_pred)\n",
        "print(confs_pred.shape)\n",
        "print(classes_pred.shape)\n",
        "# print(np.argmax(pred))\n",
        "# model.summary()"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1046, 4)\n",
            "(1046, 4)\n",
            "computation_time =  0.6356630325317383\n",
            "(233,)\n",
            "(233,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxdvzX1cln9P"
      },
      "source": [
        "list_output_layer = [[54, 'block_5_add'], [116, 'block_12_add'], [143, 'block_15_add'], [151, 'block_16_project_BN' ], [159, 'block_17_project_BN'], [167, 'block_18_project_BN']]\n",
        "for i, layer in list_output_layer:\n",
        "  print(str(i) + '  ' + layer + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "1JGnc3d9njLK",
        "outputId": "d0ef81f0-2d03-44a6-eff2-3be86a703e10"
      },
      "source": [
        "a = np.array([1, 2, 3])\n",
        "b = np.array([1, 2, 3, 4])\n",
        "c = np.array([1, 2, 3, 4, 5])\n",
        "d = np.stack([a,b], axis=2)\n",
        "d = np.stack([d,c], axis=2)\n",
        "\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-f437ae1051cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDkveJSW45L5"
      },
      "source": [
        ""
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "hy7JMpAndyfI",
        "outputId": "4656f123-c37e-40fe-8afe-ca86d4308b79"
      },
      "source": [
        "box_a = [0, 20, 0, 20]\n",
        "box_b = [30, 50, 30, 50]\n",
        "im = np.zeros((60, 60))\n",
        "im = cv.rectangle(im, (box_a[0], box_a[2]), (box_a[1], box_a[3]), 255, 1)\n",
        "im = cv.rectangle(im, (box_b[0], box_b[2]), (box_b[1], box_b[3]), 255, 1)\n",
        "imshow(im)\n",
        "overlapp = compute_overlapp(box_a, box_b)\n",
        "print(overlapp)\n"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADwAAAA8CAAAAAAfl4auAAAAR0lEQVR4nO2VQQoAIAgEM/z/l+0DG0oSFMwc1fGmazEEpooCJcuFglmcQ0ZGRt7j5Z8DcEKShxcz9NOTREZGfkr2pE+GQo8FFFMITSRmLfIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=60x60 at 0x7F7AD0B6E240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpDA5Ft7hGyG",
        "outputId": "3531fa20-37f4-4bd1-ee1a-f92b212b7f9a"
      },
      "source": [
        "784+196+49+16+1"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1046"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    }
  ]
}